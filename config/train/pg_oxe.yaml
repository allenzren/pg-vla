defaults:
  - _self_
hydra:
  run:
    dir: ${log_dir}
_target_: src.agent.train.TrainAgent

log_dir: ${oc.env:VLA_LOG_DIR}/${name}/${now:%Y-%m-%d}_${now:%H-%M-%S}_${seed}
name: paligemma_${data.train.dataset_mix}_tp${horizon_steps}
device: cuda
n_nodes: 1
seed: 42
pretrained_model_path: /n/fs/llm-unc/.cache/paligemma-3b-pt-224
load_pretrained_weights: True
resume_checkpoint_path:
train_vlm: True
action_expert_adaptive_mode:  # adaLN, adaLN-Zero, or None
offload_optimizer: False
quantize: False
lora: False
lora_r: 32
lora_dropout: 0.0
debug: False

data:
  val:  # stil aplying image randomization
    split:
    shuffle_buffer_size: 5000
  train:
    dataset_mix: bridge
    split: train[:95%]
    data_path: /n/fs/llm-unc/data/resize_224
    window_size: ${cond_steps}
    action_horizon: ${horizon_steps}
    skip_unlabeled: True
    load_proprio: True
    shuffle_buffer_size: 100000
    num_parallel_calls: 100
    traj_transform_threads: 10
    traj_read_threads: 10

wandb:
  entity: ${oc.env:VLA_WANDB_ENTITY}
  project: train-vla
  run: ${now:%H-%M-%S}_${name}

log_freq: 16
n_epochs: 10
n_updates: ${eval:'2195527 // ${global_batch_size} * ${n_epochs}'}
save_model_freq: ${eval:'2195527 // ${global_batch_size} * 1'}
eval_freq: 2000
eval_size: 1024
eval_thresholds: [0.05, 0.1, 0.2]

global_batch_size: 1024
per_device_batch_size: 16
action_lr: 5e-5
vlm_lr: 5e-5
action_lr_scheduler:
  first_cycle_steps: 10000000
  min_lr: 1e-8
  warmup_steps: 200
vlm_lr_scheduler:
  first_cycle_steps: 10000000
  min_lr: 1e-8
  warmup_steps: 200
action_weight_decay: 0
vlm_weight_decay: 0
max_grad_norm: 1.0

num_inference_steps: 10
final_action_clip_value: 1.0  # data normalized in [-1,1]

cond_steps: 1
horizon_steps: 4
action_dim: 7 # EEF_POS
proprio_dim: 7  # POS_EULER
max_seq_len: 276  # fixed 256 for image + max 20 for text
tokenizer_padding: max_length # instead of truncating to longest

image_text_hidden_size: 2048  # gemma
proprio_hidden_size: 512
action_hidden_size: 1024
time_hidden_size: 256 # if using adaptive

# Fixed
image_token_index: 257152
vocab_size: 257216
pad_token_id: 0

vision:
  _target_: src.model.paligemma.siglip.SiglipVisionModel
  config:
    hidden_size: 1152 # siglip
    intermediate_size: 4304
    num_hidden_layers: 27
    num_attention_heads: 16
    num_channels: 3
    image_size: 224
    patch_size: 14
    layer_norm_eps: 1e-6
    attention_dropout: 0.0
    num_image_tokens: 256
    lora_r: ${lora_r}
    lora_dropout: ${lora_dropout}
  quantize: ${quantize}
  lora: ${lora}

vision_projector:
  _target_: src.model.paligemma.siglip.PaliGemmaMultiModalProjector
  config:
    lora_r: ${lora_r}
    lora_dropout: ${lora_dropout}
    vision_config:
      hidden_size: 1152
      projection_dim: ${image_text_hidden_size}
  quantize: ${quantize}
  lora: ${lora}

joint:
  _target_: src.model.vla.modules.JointModel
  config:
    action_expert_adaptive_mode: ${action_expert_adaptive_mode}
    time_hidden_size: ${time_hidden_size}
    hidden_sizes:
      - ${image_text_hidden_size}
      - ${proprio_hidden_size}
      - ${action_hidden_size}
    intermediate_sizes: # mlp hidden size
      - 16384 # gemma
      - 1024
      - 4096
    lora_r: ${lora_r}
    lora_dropout: ${lora_dropout}
    #
    num_hidden_layers: 18
    num_attention_heads: 8
    num_key_value_heads: 1
    head_dim: 256
    max_position_embeddings: 8192
    rms_norm_eps: 1e-6
    rope_theta: 10000.0
    attention_bias: False
    attention_dropout: 0.0
    pad_token_id: ${pad_token_id}
  quantize: ${quantize}
  lora: ${lora}
